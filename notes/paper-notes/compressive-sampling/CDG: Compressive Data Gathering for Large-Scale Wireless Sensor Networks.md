# CDG: Compressive Data Gathering for Large-Scale Wireless Sensor Networks

## Problem/Goal

"compress data readings to reduce global data traffic" 

for that, find a transform domain for which the signal is sparse 

other domains: time domain, wavelet, DCT 

- distribute energy consumption evenly
  - classical data transmission in multihop sensor networks depletes sensor
   nodes which are closer to the sink node faster

compress-with-transmission and not compress-then-transmit 

## Basics of Algorithm

correlated sensor readings have to be transmitted jointly to increase
efficiency

- N nodes in tree, sink wants to collect M measurements
- M << N ; all individual N readings have to be restored from M

A measurement is an equation which can be solved y_i = \phi_i * d_i with d_i
being the sensed value

every node sends it's readings, multiplied with a random coefficient phi_i, to
their parent node

sink receives a weighted sum of all the readings

- parent nodes add their readings, multiplied with a random number to the sum
  of the reading of their children
  - this ensures that all nodes expend the same amount of energy
  - every node receives a reading, reads it's own sensor reading, does one
    multiplication and one addition, and sends the new reading

the sink node will then transmit the weighted sum of all readings in the
subtree to the user

the sink can compute the matrix of random numbers as it sends out the seed to
generate a seed at each node

each node generates a seed from the seed of the sink node and it's own unique
identification number

nodes then use the seed to get a pseudo random number  

the random numbers can be reproduced by the sink

sink reconstructs sensor readings by solving an l1-minimization problem

all nodes send messages so the transmission load is spread out uniformly

if nodes produce multiple readings, i.e. temperature and humidity, the values
are treated and processed seperately (or handled as a vector)

## Experimental Results

testing was done using the simulation tool ns-2 

a testbed of 1000 nodes is created, where the distance between any two nodes is
10m 

topologies used: chain and grid 

cdg was tested against baseline transmissions 

testing of network capacity with a chain network 

reducing the input interval reduces the output interval and increases package
loss ratio 

cdg had input and output intervals 5 times lower than the baseline transmission
which is good 

testing of network capacity with a grid network 

the input and output intervals of cdg are 2.3 times lower than baseline
transmitting  

further testing was done with real data sets to show sparseness in sensor
readings in the real world 

when there are more measurements than the sparseness indicator K (i.e. M > K)
the precision of the signal reconstruction by cgd is higher than 98% 

another experiment was conducted with sensor data generated by sensors in a
datacenter 

data was analyzed offline and the singal was not sparse 

re-organizing the data by ordering it by the sensed values ascending, the
authors managed to make the signal a sparse one while simultaneously capturing
outliers and keeping the high precision of of the previous experiment 

periodically, the sink would request a measurement set of M =  N meaning every
measurement to re-order the sensed values 

## Use Cases

## Advantages/Disadvantages

advatages: 

even with little spatial correlation, CDG can reduce traffic on bottleneck
nodes 

can deal with abnormal sensor readings gracefully 

sensor readings are split up in abnormal and normal readings 

however for the experiment with the data center data, the algorithm had to be
adapted  

data reconstruction is not sensitive to packet loss 

disadvatages: 

however, some spatial correlation is required, compressed sensing is not
possible 

if sensing values are not sparse in any known domain or any ordering, then the
algorithm cannot be used 

decoding complexity becomes very high with N > 1000 

not suitable for small sensor networks (can be seen as an advatage that it is
suitable for large SNs) 

routing structure has to be stable, otherwise will the controlling overhead
induced by node failures neutralize the gain of the algorithm 

## Compability

## Notes

"Instead of receiving individual sensor readings, the sink will be sent a few
weighted sums of all the readings, from which to restore the original data" 

Related Work: 

Conventional Compression 

Joint Entropy Coding 

"Conventional compression techniques utilize the correlation during the
encoding process and require explicit data communication among sensors" 

data correlation is utilized only unidirectionally 

Distributed Wavelet Transform 

same as with conventional, but nodes communicate multidirectionally with each
other  

Clustered Aggregation 

groups sensor on sensing values 

transmits only one reading per group with error boundry restriction 

Problems with Conventional Compression techniques: 

compression and routing algorithms need to be optimized which is a NP-hard
problem 

entropy based coding techniques require a lot of computational power and
transform based techniques require a lot of data exchanges which both produce a
high energy overhead 

Distributed Source Coding 

based on Slepian Wolf coding theory 

prerequisite: global correlation structure needs to be known 

sensor readings are encoded separately and sent on the shortest path to the
sink node 

static correlation patterns are assumed, abnormal readings would increase error
of decoding 

Slepian Wolf coding theory 

"compression of correlated readings, when separately encoded, can achieve same
efficiency as if they are jointly encoded, provided that messages are jointly
decoded" 
